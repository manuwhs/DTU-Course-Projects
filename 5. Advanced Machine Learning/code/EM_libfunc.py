
from scipy.special import hyp1f1
from scipy.special import gamma
import numpy as np
import copy 
import HMM_libfunc2 as HMMlf

import Watson_distribution as Wad
import Watson_sampling as Was
import Watson_estimators as Wae
import general_func as gf

def get_responsabilityMatrix2(X,theta, pimix):
    mus = theta[0]
    kappas = theta[1]
    N, D = X.shape
    D,K = mus.shape
    r = np.zeros((N,K))
    
    for i in range(N):  # For every sample
        # For every  component
        for k in range (K):
            k_component_pdf = Wad.Watson_pdf(X[i,:], mus[:,k], kappas[:,k])
            
#                print pimix[:,k]
#                print k_component_pdf
            r[i,k] = pimix[:,k]*k_component_pdf
            
        # Normalize the probability of the sample being generated by the clusters
        Marginal_xi_probability = np.sum(r[i,:])
        r[i,:] = r[i,:]/Marginal_xi_probability
    return r


def get_responsabilityMatrix_log(X,theta, pimix): 
    mus = theta[0]
    kappas = theta[1]
    N, D = X.shape
    D,K = mus.shape
    r_log = np.zeros((N,K))
    
    # Truco del almendruco
    cp_logs = []
    for k in range(K):
        cp_logs.append(Wad.get_cp_log(D,kappas[:,k]))
        
    r_log = np.zeros((N,K))
    
    # For every  component
    k_component_pdf = Wad.Watson_K_pdf_log(X[:,:].T, mus[:,:], kappas[:,:], cps_log = cp_logs)
    r_log = k_component_pdf  +  np.log(pimix[:,:]) 
    
    for i in range(N):  # For every sample
        # Normalize the probability of the sample being generated by the clusters
        Marginal_xi_probability = HMMlf.sum_logs(r_log[i,:])
        r_log[i,:] = r_log[i,:]- Marginal_xi_probability

    return r_log


def get_responsabilityMatrix_log2(X,theta, pimix):
    mus = theta[0]
    kappas = theta[1]
    N, D = X.shape
    D,K = mus.shape
    r_log = np.zeros((N,K))
    
    # Truco del almendruco
    cp_logs = []
    for k in range(K):
        cp_logs.append(Wad.get_cp_log(D,kappas[:,k]))
    
    def get_custersval(k):
        return Wad.Watson_pdf_log(X[i,:], mus[:,k], kappas[:,k], cp_log = cp_logs[k]) + np.log(pimix[:,k])

    krange = range(K)
    for i in range(N):  # For every sample
        # For every  component
        r_log[i,:] = np.array(map(get_custersval,krange)).flatten() 
        # Normalize the probability of the sample being generated by the clusters
        Marginal_xi_probability = HMMlf.sum_logs(r_log[i,:])
        r_log[i,:] = r_log[i,:]- Marginal_xi_probability
        
    return r_log
def EMdecode(Xdata, theta, pimix):
    # This function will tell you for each point in the data set the most likely point that generated it
    n,d = Xdata.shape
    d,K = Xdata.shape
    
    r_log = get_responsabilityMatrix_log(Xdata,theta,pimix)
    
    Sel = np.argmax(r_log, axis = 1)
    
    return Sel

def get_EM_Incomloglike(theta,pimix,X):

    mus = theta[0]
    kappas = theta[1]
    print kappas
    N = X.shape[0] # Number of IDD samples
    K = kappas.size
    # Calculate log-likelihood
    ll = 0;
    for i in range (N):
        aux = 0;
        for k in range(K):
            k_component_pdf = Wad.Watson_pdf(X[i,:], mus[:,k], kappas[:,k])
            aux = aux +  pimix[:,k]*k_component_pdf

        ll = ll+ np.log(aux);
    
    return ll

def get_EM_Incomloglike_log(theta,pimix,X):

    mus = theta[0]
    kappas = theta[1]
    N = X.shape[0] # Number of IDD samples
    D = X.shape[1]
    K = kappas.size
    # Calculate log-likelihood
    # Truco del almendruco
    cp_logs = []
    for k in range(K):
        cp_logs.append(Wad.get_cp_log(D,kappas[:,k]))
 
    # For every  component
    k_component_pdf = Wad.Watson_K_pdf_log(X[:,:].T, mus[:,:], kappas[:,:], cps_log = cp_logs)
    r_log = np.log(pimix[:,:]) + k_component_pdf
    
    ll = 0
    for i in range(N):  # For every sample
#        print "r_log"
#        print r_log[i,:]
        ll += HMMlf.sum_logs(r_log[i,:])  # Marginalize clusters and product of samples probabilities!!
        # Normalize the probability of the sample being generated by the clusters
#        Marginal_xi_probability = HMMlf.sum_logs(r_log[i,:])
#        r_log[i,:] = r_log[i,:]- Marginal_xi_probability
    return  ll

#    ll = 0
#    for i in range (N):
#        aux = [];
#        for k in range(K):
#            k_component_pdf = Wad.Watson_pdf_log(X[i,:], mus[:,k], kappas[:,k], cp_log = cp_logs[k])
#            aux.append( np.log(pimix[:,k]) + k_component_pdf)
#        ll = ll + HMMlf.sum_logs(aux)

def get_EM_Incomloglike_byCluster_log(theta,pimix,X):
    # Gets the incomloglikelihood by clusters of all the samples

    mus = theta[0]
    kappas = theta[1]
    N = X.shape[0] # Number of IDD samples
    D = X.shape[1] # Number of dimesions of samples
    K = kappas.size # Number of clusters
    # Calculate log-likelihood
    # Truco del almendruco
    cp_logs = []
    for k in range(K):
        cp_logs.append(Wad.get_cp_log(D,kappas[:,k]))

    k_component_pdf = Wad.Watson_K_pdf_log(X[:,:].T, mus[:,:], kappas[:,:], cps_log = cp_logs)
    r_log = np.log(pimix[:,:]) + k_component_pdf

    # Normalize probabilities first ?
    for i in range(N):  # For every sample
        Marginal_xi_probability = HMMlf.sum_logs(r_log[i,:])
        r_log[i,:] = r_log[i,:] - Marginal_xi_probability
    

    clusters = np.argmax(r_log, axis = 1) # Implemented already
#    print clusters
    pi_estimated = []
    
    for i in range (K):
        pi_i = np.where(clusters == i)[0].size
        pi_estimated.append( np.array(pi_i)/float(N))
    
#    For ll in logll
#    
#        Logglikelihoods[clusters[i]] += ll

#    r_log = np.exp(r_log)
    ll = np.sum(r_log, axis = 0).reshape(1,K)

    # Mmm filter ?
#    ll = []
#    for i in range (K):
#        ll_i = np.sum(r_log[np.where(clusters == i)[0],i])
#        ll.append(ll_i)
#    ll = np.array(ll).reshape(1,K)
#    pi_estimated = np.array(pi_estimated).reshape(1,K)
#    ll = np.exp(ll)
#    print pi_estimated
    return  ll   #ll
    
def get_r_and_ll_old(X,theta, pimix):
    # Combined funciton to obtain the loglikelihood and r in one step
    mus = theta[0]
    kappas = theta[1]
    N, D = X.shape
    D,K = mus.shape
    r_log = np.zeros((N,K))
    
    # Truco del almendruco
    cp_logs = []
    for k in range(K):
        cp_logs.append(Wad.get_cp_log(D,kappas[:,k]))
    
    ll = 0
    for i in range(N):  # For every sample
        # For every  component
        for k in range (K):
            
            k_component_pdf = Wad.Watson_pdf_log(X[i,:], mus[:,k], kappas[:,k], cp_log = cp_logs[k])
            r_log[i,k] = np.log(pimix[:,k]) + k_component_pdf
            
        ll += HMMlf.sum_logs(r_log[i,:])
        # Normalize the probability of the sample being generated by the clusters
        Marginal_xi_probability = HMMlf.sum_logs(r_log[i,:])
        r_log[i,:] = r_log[i,:]- Marginal_xi_probability
    return r_log, ll

def get_r_and_ll_old2(X,theta, pimix):
    # Combined funciton to obtain the loglikelihood and r in one step
    mus = theta[0]
    kappas = theta[1]
    N, D = X.shape
    D,K = mus.shape
    r_log = np.zeros((N,K))
    
    # Truco del almendruco
    cp_logs = []
    for k in range(K):
        cp_logs.append(Wad.get_cp_log(D,kappas[:,k]))
    
    ll = 0
    for i in range(N):  # For every sample
        # For every  component
        k_component_pdf = Wad.Watson_K_pdf_log(X[[i],:].T, mus[:,:], kappas[:,:], cps_log = cp_logs)
        r_log[i,:] = np.log(pimix[:,:]) + k_component_pdf
            
        ll += HMMlf.sum_logs(r_log[i,:])
        # Normalize the probability of the sample being generated by the clusters
        Marginal_xi_probability = HMMlf.sum_logs(r_log[i,:])
        r_log[i,:] = r_log[i,:]- Marginal_xi_probability
    return r_log, ll

def get_r_and_ll(X,theta, pimix):
    # Combined funciton to obtain the loglikelihood and r in one step
    mus = theta[0]
    kappas = theta[1]
    N, D = X.shape
    D,K = mus.shape
    r_log = np.zeros((N,K))
    
    # Truco del almendruco
    cp_logs = []
    for k in range(K):
        cp_logs.append(Wad.get_cp_log(D,kappas[:,k]))
    
    ll = 0

        # For every  component
    k_component_pdf = Wad.Watson_K_pdf_log(X[:,:].T, mus[:,:], kappas[:,:], cps_log = cp_logs)
    r_log[:,:] = np.log(pimix[:,:]) + k_component_pdf
    
    for i in range(N):  # For every sample
        ll += HMMlf.sum_logs(r_log[i,:])  # Marginalize clusters and product of samples probabilities!!
        # Normalize the probability of the sample being generated by the clusters
        Marginal_xi_probability = HMMlf.sum_logs(r_log[i,:])
        r_log[i,:] = r_log[i,:]- Marginal_xi_probability
    return r_log, ll

def init_EM_params(D,K, pi_init = None, theta_init = None, Kappa_max = 20):
    # Here we will initialize the  parameters of the mixture model, that is,the
    # theta vectors of the K components and mixing coefficients %
    
    # MIXING COEFICIENTS
    # We set the mixing coefficients with uniform discrete distribution, this
    # way, the a priori probability of any vector to belong to any component is
    # the same.
    
    if (type(pi_init) == type(None)): # If not given an initialization
        pimix = np.ones((1,K));
        pimix = pimix*(1/float(K));
    else:
        pimix = np.array(pi_init).reshape(1,K)
#        print "grgerhbwe"
    # THETA PARAMETERS OF THE K COMPONENTS
    # Give random values to the theta parameters (parameters of the distribution)
    # In this case the parameters are theta = (mu, kappa)
    
    if (type(theta_init) == type(None)): # If not given an initialization
        mus = np.random.randn(D,K);
        mus = gf.normalize_module(mus.T).T
#        print mus
        kappas = np.random.uniform(-1,1,K) * Kappa_max
        kappas = kappas.reshape(1,K)
    else:
        mus = np.array(theta_init[0]).reshape((D,K))
        kappas = np.array(theta_init[1]).reshape((1,K))
    
    theta = [mus, kappas]
    
    return pimix, theta
    
def get_pimix(r):
    N,K = r.shape
    pimix = np.zeros((1,K))
    for k in range(K):
        pimix[:,k] = np.sum(r[:,k])/N;

    return pimix

def get_theta(r, X, theta, Kappa_max = 1000):
    # We only need the old mus for checking the change of sign
    # Maybe in the future get this out
    N,d = X.shape
    N,K = r.shape
    
#    print r.shape
    kappas = np.zeros((1,K))
    mus = theta[0]
    for k in range(K):
        # We compute the weighed values given by the Momentum of the Exponential family
        # We compute the Weighted correlation matrix for the component
        # Sk = (N*pimix_k) * B_ik (*) X*X.T
        
        rk = r[:,[k]]  # Responsabilities of all the samples for that cluster
        
        ## Improvement for visualizing 
        # Since the polarity of mu does not count, and we dont want it to be changing
        # randomly, we need a way to keep it as most as possible in one side,
        # for that we multiply it with the previous, if pol = +, we keep it, if -, we change
#        new_mu = Wae.get_Weighted_MLMean(rk,X)  
        ## TODO: Check the imaginarity
        
        try:
            new_mu, kappas[:,k] = Wae.get_Watson_muKappa_ML(X, rk)
    #        print new_mu
            
        except RuntimeError as err:
            error_type = err.args[1]
            print err.args[0] % err.args[2]
            print """We saturate kappa to %f. But in the next estimation the estimated kappa will also be as bad"""% (Kappa_max)
            
            # TODO: This could not work if the Kappa_max is still to high
            
            new_mu_pos, new_mu_neg = Wae.get_Watson_mus_ML(X, rk)
            if (theta[1][0,k] >= 0):
                new_mu = new_mu_pos
            else:
                new_mu = new_mu_neg
            
            kappas[:,k] = Kappa_max # * kappas[:,k]/np.abs(kappas[:,k])
            
            
        signs = np.sum(np.sign(new_mu *  mus[:,k]))
#            print signs
        if (signs < 0):
            mus[:,k] = -new_mu
        else:
            mus[:,k] = new_mu
            
#        kappas[:,k] = Wae.get_Weighted_MLkappa(rk,mus[:,k],X)
        
    theta = [mus, kappas]
    
    return theta

def remove_cluster(theta, pi, k):
    # This function removed the cluster k from the parameters
    kappa = theta[1][0,k]
    theta[0] = np.delete(theta[0], k, axis = 1)
    theta[1] = np.delete(theta[1], k, axis = 1)
    pi = np.delete(pi, k, axis = 1)
    pi = pi / np.sum(pi)
    print "$$$$$$$$$$$$ cluster %i removed" % (k)
    print " Its kappa was %f"%(kappa)
    
    return theta, pi
#    print theta[0].shape
def manage_clusters(X, theta, pi, Kappa_max = 1000):
    kappas = theta[1]
    K = kappas.shape[1]
    Nsam,D = X.shape
    clusters_change = 0
    # Truco del almendruco
    kummer_check = []
    for k in range(K):
        try:
            kummer_check.append(Wad.get_cp_log(D,kappas[:,k]))
        except RuntimeError as err:
            print "Error in Managing clusters"
            error_type = err.args[1]
            print err.args[0] % err.args[2]
            print """We saturate kappa to %f. But in the next estimation the estimated kappa will also be as bad"""% (Kappa_max)
            
            # TODO: This could not work if the Kappa_max is still to high
            kappas[:,k] = Kappa_max * kappas[:,k]/np.abs(kappas[:,k])
            clusters_change = 1
    # We go backwards in case we erase, not to affect indexes
    for k in range(K):
        
        if (np.abs(kappas[0,K - 1 -k]) > 1000):
            pass
#            theta, pi = remove_cluster(theta,pi,K - 1 -k)
            
#        if (kummer_check[K - 1 -k] == 0): # If we fucked up
#            theta, pi = remove_cluster(theta,pi,K - 1 -k)

    # Maybe condition on pi as well ? If pi is too small.

#    K = theta[0].shape[1]
#    for k in range(K):
#        if (pi[0,K - 1 -k] <  0.02/K):  # Less than 5 percent of the uniform # (1/float(Nsam)
#            theta, pi = remove_cluster(theta,pi,K - 1 -k)
#    
    return theta,pi, clusters_change
    