{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Fundamentals\n",
    "> <span style=\"color:gray\"> Created by Toke Faurby ([faur](https://github.com/Faur)).\n",
    "\n",
    "This Jupyter Notebook contains a very fast introduction to TensorFlow (TF).\n",
    "This introduciton is **VERY** fast, and leaves many topics untouched or under touched, but should give you enough of an introduction to be able to follow the later notebooks.\n",
    "\n",
    "TensorFlow is an open source software library for numerical computation that can be used for many things, but is mostly know for its use for deep and machine learning applications.\n",
    "Since its release in 2015 it has quickly become one of the most popular and most actively developed libraries for deep learning.\n",
    "TensorFlow represents computations as graphs.\n",
    "This added layer of abstraction makes TensorFlow very flexible, and allows the same code to be optimized and parallelized for many different types of hardware such as GPUs, clusters, or mobile phones.\n",
    "\n",
    "Some details of TensorFlow can be a bit confusing, so don't get discouraged if you don't get it right away.\n",
    "You'll pick them up when you worked with it for some time.\n",
    "\n",
    "#### External resources\n",
    "If you want a deeper dive the following are good places to start:\n",
    "\n",
    "* [Official getting started material](https://www.tensorflow.org/get_started/) - collection of good tutorials from beginer to very advanced.\n",
    "* [API documentation](https://www.tensorflow.org/api_docs/python/) - Most of the documentation for TF is written into the code, so the best way to figure out how somethings works is often to look it up in the API, and then look at the implementation. The [API guides](https://www.tensorflow.org/api_guides/python/array_ops) can also be very useful sometimes.\n",
    "* [Keynote (TensorFlow Dev Summit 2017)](https://www.youtube.com/watch?v=4n1AHvDvVvw&t=33s). **30 min video** describing where TensorFlow is right now, and the underlying design principals.\n",
    "* [TensorFlow at DeepMind (TensorFlow Dev Summit 2017)](https://www.youtube.com/watch?v=VdDmhOCw6J0). **20 min video** describing how TensorFlow is used at DeepMind, and some of the results they have been able to achieve with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow\n",
    "TensorFlow provides multiple APIs and can be interfaced using different languages, with Python being the most common, and the focus of this course.\n",
    "The lowest level API, **TensorFlow Core**, provides you with fine-grained control.\n",
    "Higher level APIs, such as `Keras` or `tf.contrib.learn`, are built on top of TensorFlow Core.\n",
    "They help manage data, training, and inference, and are generally faster and easier to use.\n",
    "This guide begins with an introduction to TensorFlow Core.\n",
    "Later, in other notebooks, we will demonstrate how to use Tensorflow in a way that is closer to how it is used in the real world.\n",
    "\n",
    "![](images/tf_overview.png)\n",
    "\n",
    "Note: The part of TensorFlow is listed under `contrib` (short for contribution) are still in development, and their interface may change.\n",
    "The rest of TensorFlow is stable within the major release cycle (e.g. `0.x`, `1.x`, ...)\n",
    "\n",
    "To use TensorFlow you need to understand how TensorFlow:\n",
    "* Represents computations as graphs.\n",
    "* Executes graphs in the context of Sessions.\n",
    "* Represents data as tensors.\n",
    "* Uses feeds and fetches to get data into and out of arbitrary operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does TensorFlow work?\n",
    "\n",
    "The two basic building blocks of a TensorFlow graph are **tensors** and **operations** (called ops for short).\n",
    "* [**tf.Tensor**](https://www.tensorflow.org/api_docs/python/tf/Tensor): The edges in the graph\n",
    "    * Tensors are typed multi-dimensional arrays, and are used for data and parameters.\n",
    "* [**tf.Operation**](https://www.tensorflow.org/api_docs/python/tf/Operation): the nodes in the graph \n",
    "    * allows us to perform operations on tensors, resulting in new tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The basics\n",
    "\n",
    "> In part 1 we will show how some simple computations are performed in TensorFlow, and look at how the computational graph looks.\n",
    "\n",
    "### Basic operations\n",
    "\n",
    "Let us begin with a simple example -- 2D **linear regression**: $y = ax + b$.\n",
    "Where $x$ is the input, $y$ is the output, and $a,~b$ are the parameters.\n",
    "\n",
    "Before we begin we need to import TensorFlow and some other handy libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Python 2/3 compatability\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "## Import libraries\n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(os.path.join('.', '..')) # Allow us to import shared custom \n",
    "                                         # libraries, like utils.py\n",
    "import utils # contain various helper funcitons that aren't \n",
    "             # important to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters let us compute $y$ when $a=2$, and $b=-1$ for a couple of different $x$.\n",
    "\n",
    "\n",
    "We define the parameters as a special kind of Tensor, called [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable).\n",
    "The `tf.Variable` allows us to store and update Tensors in our graph.\n",
    "Variables are used to build weights for our neural network.\n",
    "Note you often want to use the wrapper [tf.get_variable](https://www.tensorflow.org/api_docs/python/tf/get_variable).\n",
    "In this notebook however we will just use `tf.Variable` as it is simpler and easier to understand.\n",
    "\n",
    "\n",
    "There is an [official tutorial](https://www.tensorflow.org/programmers_guide/variables) that does a good job of explaining `tf.Variables`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Building the computational graph\n",
    "\n",
    "# In case we have already created something: clear it\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the two variables\n",
    "a = tf.Variable(2., name=\"a\")\n",
    "b = tf.Variable(-1., name=\"b\")\n",
    "# The 'name' argument indicates what TF should call the variable internally.\n",
    "# It is a good idea to properly name your ops, as it makes debugging and later \n",
    "# analysis much easier! The name is also used when visualizing the graph with \n",
    "# TensorBoard, as we will se later.\n",
    "\n",
    "\n",
    "# Create x, and give it some numbers\n",
    "x_values = [-2, -1, 0, 1, 2]\n",
    "x_var = tf.Variable(x_values, name=\"xVariable\", dtype=tf.float32)\n",
    "\n",
    "# Define y\n",
    "with tf.name_scope('yFromVariable'): \n",
    "    y_from_var = a*x_var + b\n",
    "# tf.name_scope is used when we wish to give the same name to multiple ops\n",
    "# or tensors. E.g. here we are both adding and multplying\n",
    "\n",
    "# Print the results\n",
    "print(y_from_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happened?\n",
    "Why didn't `print(y_from_var)` print out the answer?\n",
    "\n",
    "You might have expected `y_from_var` to be the results of `a*x_var + b`, but instead we got an **Tensor object**.\n",
    "This is because TensorFlow works by first creating a computational graph representation.\n",
    "This is what we created above, i.e. the tensor `y_from_var` is an edge in the computational graph.\n",
    "To compute anything, a graph must be launched in a `Session`, which compiles the necessary functions and variables such that it can be run.\n",
    "A `Session` places the graph ops onto `Devices`, such as CPUs or GPUs, and provides methods to execute them.\n",
    "These methods return [numpy](http://www.numpy.org/) ndarray objects in Python, and as `tensorflow::Tensor` instances in C and C++.\n",
    "\n",
    "![](images/tf_input_output.png)\n",
    "\n",
    "Lets compile and run `y_from_var` now!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorFlow operations are performed by 'Sessions'\n",
    "with tf.Session() as sess: \n",
    "    # We almost always need to start by initializes the graph.\n",
    "    # This tells TensorFlow to fill in the value of the variables.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Compute y by running the operation\n",
    "    y_output = sess.run(y_from_var)\n",
    "\n",
    "print('y_output is a ' + str(type(y_output)) + '\\n')\n",
    "\n",
    "# Print the results\n",
    "print('{:4s}  {:4s}'.format('x_var', '  y'))\n",
    "for i in range(len(x_values)):\n",
    "    s = \"{:4.1f} : {:4.1f}\"\n",
    "    print(s.format(x_values[i], y_output[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada! You have now done a very simple thing in a complicated way.\n",
    "\n",
    "\n",
    "#### Placeholders\n",
    "That is all well an good, but what if we wanted to compute this for different values of $x$?\n",
    "Right now we have defined `x_var` as a `tf.Variable`.\n",
    "This makes changing it cumbersome.\n",
    "A better approach is using [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) are used to feed our input data to the computational graph.\n",
    "A placeholder can be thought of in the same way as '$x$' in the expression $f(x)=x^2$.\n",
    "Placeholers lets TensorFlow know that this Tensor is an input to the graph.\n",
    "\n",
    "A [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) has 3 important arguments (click link for full documentation):\n",
    "\n",
    "* **`dtype`** specifying what kind of data we are dealing with. Generally use `tf.float32`, as most GPU's are only optimized for 32 bit floating points.\n",
    "* **`shape`** lets TF know the dimensions of the variable. Writing `None` allows us to change the number of dimensions, without having to recompile the graph. This however prevents some optimization, so the dimensions should be specified when possible. It is common to provide `None` for the first dimension, which allows us to vary the batch size at runtime.\n",
    "* **`name`** is what TF will call the placeholder internally. For instance this is the name that will be printed in case that there is an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create x as a placeholder now!\n",
    "x_ph = tf.placeholder(dtype=tf.float32, shape=[None], name=\"xPlaceholder\")\n",
    "\n",
    "# Define another y, using the placeholder x this time\n",
    "with tf.name_scope('yFromPlaceholder'):\n",
    "    y_from_ph = a*x_ph + b\n",
    "\n",
    "x_new_values = [-0.2, -0.1, 0, 0.1, 0.2]\n",
    "\n",
    "## Compute y\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = {x_ph : x_new_values}\n",
    "    y_output = sess.run(y_from_ph, feed_dict=feed_dict)\n",
    "\n",
    "    \n",
    "# Print the results\n",
    "print('{:4s}  {:4s}'.format(' x_ph', '  y'))\n",
    "for i in range(len(x_values)):\n",
    "    s = \"{:4.1f} : {:4.1f}\"\n",
    "    print(s.format(x_new_values[i], y_output[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happened?\n",
    "\n",
    "This time, when we created the graph, we created $x$ as a `tf.placeholder`.\n",
    "This means that `x_ph` is a stand-in for real data, and will be replaced when we actually want to compute something.\n",
    "So when we want to compute `y` for a particular value we simply **feed** that value into the graph, using a `feed_dict`.\n",
    "\n",
    "If we wanted to change the values now, we simply need to feed a new value.\n",
    "In other words, tensorflow creates a separate environment which is accessed through then `Session.run` command.\n",
    "This is the case both when you want to **feed** information into the tensorflow environment and when you want to **access** information.\n",
    "\n",
    "**Sessions**, **variables** and **placeholders** can be a little hard to wrap your head around at first.\n",
    "It can therefore be a good idea to read up on these.\n",
    "In the descriptions above there are several links to useful documentation and tutorials, have a look whenever there is something you don't understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the graph with TensorBoard\n",
    "\n",
    "When you execute the cell below you should see a graph that represents the work we have done so far.\n",
    "Normally TensorBoard is opened in a separate browser window, but for now we will show it in-line.\n",
    "\n",
    "The TensorBoard graph visualizer is a great tool for examining your model.\n",
    "It is important to use `tf.name_scope` to dutifully name your variables properly!\n",
    "Otherwise the graph visualizer quickly becomes unwieldy and useless.\n",
    "This takes practice, but it is well worth it.\n",
    "Propper usage of `tf.name_scope` also makes debugging easier, so it is a good habbit to get into.\n",
    "\n",
    "In this example we embed the graph visualizer in Jupyter.\n",
    "The visualizer isn't made for this, and not all features are present.\n",
    "Normally you would access the graph visualizer through **TensorBoard**, as we do at the end of this notebook.\n",
    "If you are interested in how to embed TensorBoard in the notebook see the `../utils.py` file.\n",
    "\n",
    "#### External resources\n",
    "Using TensorBoard makes developing and monitoring deep learning models a lot.\n",
    "The official tutorials as well as the [README](https://github.com/tensorflow/tensorboard/blob/master/README.md) are very well made and go into a lot more depth about how to use TensorBoard:\n",
    "*    [Graph visualization](https://www.tensorflow.org/get_started/graph_viz), \n",
    "*    [Visualizing Learning](https://www.tensorflow.org/get_started/summaries_and_tensorboard), \n",
    "*    [Embedding Visualization](https://www.tensorflow.org/get_started/embedding_viz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click a node to see its attributes and high level information.\n",
    "**Double click a node to expand it**.\n",
    "Try double clicking on one of the y's.\n",
    "Doing so will show you the operations that are necessary to compute $y$, i.e. a multiplication and an addition.\n",
    "This is especially useful for examining the dimensions of your data as it flows through the graph (only viewable when TensorBoard is run separately).\n",
    "\n",
    "**<span style=\"color:red\">Mini-assignment:</span> **\n",
    "Try and change the argument in `tf.name_scope` when defining `y_from_var` and `y_from_ph` to \"`y_variable`\", and \"`y_placeholder`\".\n",
    "Then run code visualizing TensorBoard again.\n",
    "* Notice what changed? Can you think of when this kind of naming is smart to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Exercise 1.1: Your first TensorFlow graph</span>\n",
    "> **NB**: You should **NOT** overwrite any of the variable names, as we need them later!\n",
    "\n",
    "For the first exercise you must implement Pythagoras' famous equation:\n",
    "$$c = \\sqrt{d^2 + e^2}$$\n",
    "\n",
    "You should create $e$ and $d$ as placeholders, and then compute $c$ for $d = {3, 2, 1}$ and $e = {4,5,6}$.\n",
    "\n",
    "It is **important** that you use TF ops for all the computations on the graph.\n",
    "(e.g. use `tf.square` instead of `np.square`).\n",
    "Otherwise TF can't optimize the code properly, and you risk it becoming VERY slow.\n",
    "You can find the TF math ops that you need [here](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/basic_math_functions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_values = [3, 2, 1, -1, -2, -3]\n",
    "d_values = [4, 5, 6,  7,  8,  9]\n",
    "\n",
    "## Your code here ###\n",
    "# 1) Define the placeholders\n",
    "\n",
    "# 2) Define the operations of the graph\n",
    "\n",
    "# 3) Start a session, and compute the output\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Use this variable name as your output\n",
    "    c_output = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Print out the results, and validate you get the correct values\n",
    "true_values = np.sqrt(np.square(e_values) + np.square(d_values))\n",
    "assingment_1_success = True\n",
    "\n",
    "# Check if your c_output is the same as true_values\n",
    "for i in range(len(true_values)):\n",
    "    assingment_1_success = False if not np.abs(true_values[i] - c_output[i]) < 1e-6 else assingment_1_success\n",
    "    print('Corect value {:4.3f}, your value {:4.3f}. '.format(true_values[i], c_output[i]), end='')\n",
    "    if not assingment_1_success: \n",
    "        print(\"Oops :(\")\n",
    "        print(\"\\nSometihng went wrong, and the output isn't as expected.\\\n",
    "               \\nGo back and have a look, or ask someone for help.\")\n",
    "        break\n",
    "    print('Correct!')\n",
    "\n",
    "    \n",
    "if assingment_1_success:\n",
    "    print('\\nGood job! \\nTake a break, strecht your legs, and then continue onwards!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Mini-assignment:</span>**\n",
    "After having successfully completed the assignment go back and run the TensorBoard visualization code again.\n",
    "* How does the graph look now?\n",
    "* Did you remember to give your variables and placeholders meaningful names, or does everything look like a mess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Linear Regression\n",
    "> In part 2 we perform linear regression with gradient descent using TensorFlow.\n",
    "This example is simple, but the techneques and overarching approach is the same for more complex models.\n",
    "\n",
    "Above we used TF for things that could be done more easily in `numpy`. \n",
    "Hardly inspiring.\n",
    "Here in part 2 we will extend what we learnt in part 1 and begin introducing techniques relevant for deep learning.\n",
    "We will do this through a linear regression example, but rather than use the ordinary least squares method we will use gradient descent.\n",
    "Gradient descent is the backbone of the **backpropagation algorithm**, which is used in virtually all deep learning applications.\n",
    "\n",
    "In case you aren't familiar with gradient descent: don't worry we will cover it properly later.\n",
    "However Andrew Ng (one of the big names in AI) explains it very well in [this 10 min video](https://www.youtube.com/watch?v=F6GSRDoB-Cg) as part of his [machine learning MOOC](https://www.coursera.org/learn/machine-learning) (which is great!).\n",
    "\n",
    "But briefly put: You define a differentiable **loss function** (*loss*, *cost*, and somtimes *error* are used interchangably) that somehow measures the amount of error your model is making.\n",
    "A common approach is to use a distance measure between model predictions and the actual data.\n",
    "The model parameters are then iteratively updated in the negative direction of the gradients, thus reducing the loss function.\n",
    "\n",
    "For real valued data we often use the **mean squared error** (MSE) loss function:\n",
    "\n",
    "$$ loss = \\frac{1}{n} \\sum^n (y_{true} - y_{estimated})^2$$\n",
    "\n",
    "This is the loss function we will use in this example.\n",
    "\n",
    "The first step is to get some data.\n",
    "We will just create some artificial, well behaved data to start with.\n",
    "We create both a **training set** and a **validation set**.\n",
    "The training set is used to tune the model parameters, and the validation set is used to evaluate the model during training.\n",
    "It is common to also have a **test set** which is used for a final evaluation of the model, when the training is complete.\n",
    "We won't bother with the test set for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Creating the data\n",
    "n_train = 100\n",
    "n_valid = 20\n",
    "train_input = np.linspace(-1, 1, n_train)\n",
    "train_target = - train_input + np.random.randn(*train_input.shape) * 0.4\n",
    "\n",
    "valid_input = np.linspace(-1, 1, n_valid)\n",
    "valid_target = - valid_input + np.random.randn(*valid_input.shape) * 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the model we defined in part 1.\n",
    "Before we begin we will visualize the data and the initial (terrible) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = {x_ph : train_input}\n",
    "    y_output = sess.run(y_from_ph, feed_dict=feed_dict)\n",
    "\n",
    "plt.plot(train_input, y_output, c='k', label='Model')\n",
    "plt.scatter(train_input, train_target, c='b', alpha=0.5, label='Training set')\n",
    "plt.scatter(valid_input, valid_target, c='r', alpha=0.5, label='Validation set')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **loss function**, and the TF ops that will update the weights $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now also need a placeholder for y to compute the loss\n",
    "y_ph = tf.placeholder(dtype=tf.float32, shape=[None], name='yPlaceholder')\n",
    "\n",
    "# Define the loss function\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y_ph - y_from_ph))\n",
    "\n",
    "# Define that we wish to use gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# And finally create the op that updates our weights a and b\n",
    "# One of the (many) cool things about TF is that it automatically \n",
    "# differentiates the loss function.\n",
    "train_op = optimizer.minimize(loss, var_list=[a, b])\n",
    "\n",
    "# We can use TensorBoard to track the training progress. \n",
    "# In this case we wish to track the loss\n",
    "summary_loss = tf.summary.scalar(\"performance/loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning the training **start TensorBoard**.\n",
    "Open a terminal and `cd` to the directory that the notebook is in and type:\n",
    "\n",
    "    tensorboard --logdir=logdir --port=6006\n",
    "\n",
    "One you have run the above command open a new tab in your browser, and navigate to the '*URL*': \n",
    "\n",
    "    http://localhost:6006/\n",
    "\n",
    "`6006` is the port ID.\n",
    "The default is `6006`, but you can specify anything you want (that isn't used for something else).\n",
    "\n",
    "Now that we have everything setup, we can train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define where we want to save the TensorBoard summaries\n",
    "timestr = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "logdir = os.path.join('.', 'logdir', timestr)\n",
    "\n",
    "train_dict={x_ph: train_input, y_ph: train_target}\n",
    "valid_dict={x_ph: valid_input, y_ph: valid_target}\n",
    "\n",
    "max_epoch = 50\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # We use two summary writers. This is a hack that allows us to write \n",
    "    # show two plots in the same fiigure in TensorBoard\n",
    "    summary_writer_train = tf.summary.FileWriter(os.path.join(logdir, 'train'), sess.graph)\n",
    "    summary_writer_valid = tf.summary.FileWriter(os.path.join(logdir, 'valid'), sess.graph)\n",
    "\n",
    "    for e in range(max_epoch):\n",
    "        # Update the parameters, and compute the summary\n",
    "        train_loss, summary_train, _ = sess.run([loss, summary_loss, train_op], feed_dict=train_dict)\n",
    "        train_losses.append(train_loss)\n",
    "        # Note that we don't use the train_op on the validation set!\n",
    "        valid_loss, summary_valid = sess.run([loss, summary_loss], feed_dict=valid_dict)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Write the summaries\n",
    "        summary_writer_train.add_summary(summary_train, e)\n",
    "        summary_writer_valid.add_summary(summary_valid, e)\n",
    "\n",
    "        ## Visualize the training montage!\n",
    "        y_output = sess.run(y_from_ph, {x_ph: train_input})\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        fig.suptitle('epoch = {:2}. train error = {:4.2}. valid error = {:4.2}'.format(e, train_loss, valid_loss) )\n",
    "\n",
    "        ax[0].plot(train_input, y_output, c='k', label='Model')\n",
    "        ax[0].scatter(train_input, train_target, c='b', alpha=0.5, label='Training set')\n",
    "        ax[0].scatter(valid_input, valid_target, c='r', alpha=0.5, label='Validation set')\n",
    "        \n",
    "        ax[1].plot(range(len(train_losses)), train_losses, c='b', label='Training set')\n",
    "        ax[1].plot(range(len(valid_losses)), valid_losses, c='r', label='Validation set')\n",
    "        ax[1].set_xlim([0, max_epoch])\n",
    "        ax[1].set_ylim([0, None])\n",
    "        ax[1].legend(loc=1)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the loss changes as a function of epochs.\n",
    "This plot is called the **learning curves**.\n",
    "We often want to use TensorBoard, rather manually plotting.\n",
    "\n",
    "The learning curves can tell you a lot about how the network is doing, and how well the training is going.\n",
    "In this case they look close to ideal, that is:\n",
    "* Training and validation loss follow each other closely\n",
    "* The loss asymptotes to a *low value* (what low means depends on application)\n",
    "\n",
    "In many cases the learning curves won't be this nice, and later we will go into more depth about what can be read from them.\n",
    "\n",
    "While in TensorBoard have a look at the **graphs** tab, and look at how it changed since the first time we looked at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Mini-assignment:</span>**\n",
    "The training error will generally be worse than the validation error.\n",
    "* Why is that? \n",
    "\n",
    "*[Don't worry if you can't figure it out, we will cover this later :) ]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
