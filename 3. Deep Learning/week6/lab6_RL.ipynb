{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Exercise\n",
    "\n",
    "For this exercise we will be using the [OpenAI Gym](https://gym.openai.com/) provided by [OpenAI](https://openai.com/). To get to know Gym, you are encouraged to read this [blogpost](https://openai.com/blog/openai-gym-beta/) (~10 minutes) and refer to the [docs](https://gym.openai.com/docs) along the way.\n",
    "\n",
    "In this exercise we will train a neural network agent to navigate various environments from the OpenAI Gym.\n",
    "\n",
    "## 0. Prerequisites (if running on your own machine)\n",
    "\n",
    "We assume you already have Theano and Lasagne installed -- otherwise go back to the first exercise for instructions. \n",
    "\n",
    "Below is a brief guide on how to install OpenAI Gym. For more details please refer to the [docs](https://gym.openai.com/docs).\n",
    "   \n",
    "```\n",
    "$ cd ~/path/to/dir/...\n",
    "$ git clone https://github.com/openai/gym\n",
    "$ cd gym\n",
    "$ pip install -e . # minimal install\n",
    "```\n",
    "\n",
    "Verify your installation is working by importing `gym` and check for errors:\n",
    "\n",
    "```\n",
    "$ python\n",
    ">>> import gym\n",
    "[no errors]\n",
    "```\n",
    "\n",
    "Now restart this notebook before moving on to the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started\n",
    "\n",
    "Now that you have everythong installed, lets get started!\n",
    "\n",
    "The code below will import `gym` and initialize the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) environment. The task of this environment is to move a cart in order to balance a pole attached on top, but for now we will just take random actions for 200 timesteps to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import gym\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne.nonlinearities import tanh, softmax\n",
    "    \n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    patch = plt.imshow(frames[0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop')\n",
    "            , autoplay=True)\n",
    "\n",
    "def custom_render(state):\n",
    "    \"\"\"\n",
    "    env.render() uses pyglet, which requires GLX > 1.2, which is cumbersome\n",
    "    to get working on a server.\n",
    "    \"\"\"\n",
    "    x = int(state[0] * 50 + 128) # Position\n",
    "    theta = state[2]             # Angles in radians away from horizontal\n",
    "\n",
    "    mark_size = 3\n",
    "    cart_w = 25; cart_h = 10\n",
    "    pole_l = 90; pole_w = 4\n",
    "    \n",
    "    frame = np.zeros((256,256)) + 256\n",
    "    frame[200, :] = 0\n",
    "    frame[200-mark_size:200+mark_size, 128-mark_size:128+mark_size] = 0\n",
    "\n",
    "    image = Image.fromarray(frame)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.line((x, 200,\n",
    "               x   + np.sin(theta)*pole_l,   # x coord for top of pole\n",
    "               200 - np.cos(theta)*pole_l)   # y coord for top of pole\n",
    "              , width=pole_w, fill=0)\n",
    "    draw.line((x-cart_w, 200, x+cart_w, 200), width=cart_h, fill=128)\n",
    "    del draw\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# init and run an example environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "frames = []\n",
    "for _ in range(100):\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    frames.append(custom_render(state))\n",
    "\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was all very nice, but taking random actions doesn't really solve the task. We have to do something smarter. In the next part we will train an agent to solve the task by reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy gradient agent\n",
    "\n",
    "In this part we will create an agent that can learn to solve tasks from OpenAI Gym by applying the policy gradient method.\n",
    "\n",
    "The agent is designed to work on environments with a discrete action space. Extending the code to also handle environments with countinous action space is left as an optional exercise.\n",
    "\n",
    "But first here is a short introduction to policy gradients.\n",
    "\n",
    "### Policy gradients\n",
    "\n",
    "We want to learn a policy neural network $p_\\theta(a_{t}|s_{t-1})$ with parameters $\\theta$ for action $a_t$ given the previous state $s_{t-1}$ only.\n",
    "When the action $a$ is discrete we can implement this by a softmax output taking $s$ as input. \n",
    "The (discounted) cumulative award for a sequence terminating after $T$ time-steps is\n",
    "\n",
    "$$\n",
    "R = \\sum_{t=1}^T \\gamma^{t-1} r_{t} \\ .\n",
    "$$\n",
    "\n",
    "The expectation of $R$ over a\n",
    "policy roll-out $p_\\theta({\\bf a}|{\\bf s})$ is \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R|\\theta] = \\int R({\\bf a},{\\bf s}) p_\\theta({\\bf a},{\\bf s}) d{\\bf a} d{\\bf s}\\ ,\n",
    "$$\n",
    "\n",
    "where ${\\bf a} = a_1,\\ldots,a_T$, ${\\bf s}=s_0,\\ldots,s_T$ and\n",
    "\n",
    "$$\n",
    "p_\\theta({\\bf a},{\\bf s}) = p(s_0) \\prod_{t=1}^T \\left[ \n",
    "p(s_{t}|s_{t-1},a_t) p_\\theta(a_{t}|s_{t-1})\n",
    "\\right]\\ .\n",
    "$$\n",
    "\n",
    "In this formulation $s_t$ is a stochastic function of the previous action and state: $p(s_t|a_t,s_{t-1})$. We can draw from the joint distribution of actions and states through the environment but $p(s_t|a_t,s_{t-1})$ is unknown. \n",
    "\n",
    "A deterministic environment, think chess or go, is a special case of this set-up where the state is a deterministic function of the previous state and action: $s_t = f(a_t,s_{t-1})$. We can include the deterministic formulation in the general by using a Dirac $\\delta$-function: $p(s_t|a_t,s_{t-1})= \\delta(s_t - f(a_t,s_{t-1}))$. The integration over $s_1,\\ldots,s_T$ may be carried out explicitly:    \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R|\\theta] = \\int R({\\bf a},s_0) p(s_0) \\prod_{t=1}^T  p_\\theta(a_{t}|s_{t-1})d{\\bf a} ds_0\\ ,\n",
    "$$\n",
    "\n",
    "where $s_{t-1}=f(a_{t-1},s_{t-2})$.\n",
    "\n",
    "We use gradient ascent to learn an approximation to a policy that maximizes the cumulative reward. \n",
    "So we need to compute the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] = \\int R({\\bf a}, {\\bf s}) \\nabla_\\theta p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a}d{\\bf s} \\ .\n",
    "$$\n",
    "\n",
    "We can now use the identity\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta p_\\theta({\\bf a},{\\bf s}) = p_\\theta({\\bf a},{\\bf s}) \\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})\n",
    "$$\n",
    "\n",
    "to express the gradient as an average over $p_\\theta({\\bf a},{\\bf s})$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] = \\int p_\\theta({\\bf a},{\\bf s}) ( R({\\bf a}, {\\bf s}) - b ) \\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s}) d{\\bf a}d{\\bf s}\\ .\n",
    "$$\n",
    "\n",
    "The constant factor $b$ will not affect the gradient but will needed in practice when we estimate gradients by Monte Carlo (that is roll-outs). We can prove that subtracting $b$ will not change the gradient by using the identity from above again:\n",
    "\n",
    "\\begin{align*}\n",
    "0 & = \\nabla_\\theta 1 \\\\\n",
    "  & = \\nabla_\\theta \\int p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a} d{\\bf s}\\\\\n",
    "  & = \\int \\nabla_\\theta p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a}d{\\bf s}\\\\\n",
    "  & = \\int p_\\theta({\\bf a},{\\bf s}) \\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a}d{\\bf s} \\ .\n",
    "\\end{align*}\n",
    "\n",
    "We cannot evaluate the average over roll-outs analytically but we have an environment simulator that when supplied with our current policy $p_\\theta(a|s)$ can return the sequence of action, states and rewards. This allows us to replace the integral by a Monte Carlo average over $V$ roll-outs\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{V} \\sum_{v=1}^V ( R({\\bf a}^{(v)}, {\\bf s}^{(v)}) - b) \\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})\n",
    "$$\n",
    "\n",
    "Note also that the gradient of $\\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})$ does not depend explicitly on the state distribution:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)}) = \\sum_{t=1}^T \\nabla_\\theta \\log p_\\theta(a_{t}|s_{t-1}) \\ .\n",
    "$$\n",
    "\n",
    "We are almost done. As a last step we will use the freedom in the choice of $b$ to select a $b$ that will make the Monte Carlo estimate of the gradient have the lowest possible variance. In other words, the finite Monte Carlo sample give us a noisy gradient and by this correction we can make it vary as little as possible between roll-out draws.\n",
    "\n",
    "The $b$ that minimizes the variances can be found from minimizing the following expression:\n",
    "\n",
    "$$\n",
    "\\int p_\\theta({\\bf a},{\\bf s}) \\, ( R({\\bf a}, {\\bf s}) - b )^2 \\,  |\\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})|^2 \\,  d{\\bf a}d{\\bf s} - \\left( \\nabla_\\theta \\mathbb{E}[R|\\theta] \\right)^2 \\ .\n",
    "$$\n",
    "\n",
    "The solution to this problem is\n",
    "\n",
    "$$\n",
    "b = \\frac{\\int p_\\theta({\\bf a},{\\bf s}) \\, R({\\bf a}, {\\bf s}) \\,  |\\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})|^2 d{\\bf a}d{\\bf s}}{\\int p_\\theta({\\bf a},{\\bf s}) \\, |\\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})|^2 d{\\bf a}d{\\bf s}} \\ .\n",
    "$$\n",
    "\n",
    "We replace this expression by a Monte Carlo average:\n",
    "\n",
    "$$\n",
    "b = \\frac{\\sum_{v=1}^V  R({\\bf a}^{(v)}, {\\bf s}^{(v)}) \\, |\\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})|^2}{\\sum_{v=1}^V | \\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})|^2}\n",
    "$$\n",
    "\n",
    "In the code below we instead use a time-step dependent baseline correction, $b(s_t)$, as described in [here](https://gym.openai.com/docs/rl#policy-gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    Reinforcement Learning Agent\n",
    "    \n",
    "    This agent can learn to solve reinforcement learning tasks from\n",
    "    OpenAI Gym by applying the policy gradient method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        # symbolic variables for state, action, and advantage\n",
    "        sym_state = T.fmatrix()\n",
    "        sym_action = T.ivector()\n",
    "        sym_advantage = T.fvector()\n",
    "        # policy network\n",
    "        l_in = InputLayer(shape=(None, n_inputs))\n",
    "        l_hid = DenseLayer(incoming=l_in, num_units=20, nonlinearity=tanh, name='hiddenlayer')\n",
    "        l_out = DenseLayer(incoming=l_hid, num_units=n_outputs, nonlinearity=softmax, name='outputlayer')\n",
    "        # get network output\n",
    "        eval_out = lasagne.layers.get_output(l_out, {l_in: sym_state}, deterministic=True)\n",
    "        # get trainable parameters in the network.\n",
    "        params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "        # get total number of timesteps\n",
    "        t_total = sym_state.shape[0]\n",
    "        # loss function that we'll differentiate to get the policy gradient\n",
    "        loss = -T.log(eval_out[T.arange(t_total), sym_action]).dot(sym_advantage) / t_total\n",
    "        # learning_rate\n",
    "        learning_rate = T.fscalar()\n",
    "        # get gradients\n",
    "        grads = T.grad(loss, params)\n",
    "        # update function\n",
    "        updates = lasagne.updates.sgd(grads, params, learning_rate=learning_rate)\n",
    "        # declare training and evaluation functions\n",
    "        self.f_train = theano.function([sym_state, sym_action, sym_advantage, learning_rate], loss, updates=updates, allow_input_downcast=True)\n",
    "        self.f_eval = theano.function([sym_state], eval_out, allow_input_downcast=True)\n",
    "    \n",
    "    def learn(self, env, n_epochs=100, t_per_batch=10000, traj_t_limit=None,\n",
    "              learning_rate=0.1, discount_factor=1.0, n_early_stop=0):\n",
    "        \"\"\"\n",
    "        Learn the given environment by the policy gradient method.\n",
    "        \"\"\"\n",
    "        self.mean_train_rs = []\n",
    "        self.mean_val_rs = []\n",
    "        self.loss = []\n",
    "        for epoch in xrange(n_epochs):\n",
    "            # 1. collect trajectories until we have at least t_per_batch total timesteps\n",
    "            trajs = []; t_total = 0\n",
    "            while t_total < t_per_batch:\n",
    "                traj = self.get_trajectory(env, traj_t_limit, deterministic=False)\n",
    "                trajs.append(traj)\n",
    "                t_total += len(traj[\"r\"])\n",
    "            all_s = np.concatenate([traj[\"s\"] for traj in trajs])\n",
    "            # 2. compute cumulative discounted rewards (returns)\n",
    "            rets = [self._cumulative_discount(traj[\"r\"], discount_factor) for traj in trajs]\n",
    "            maxlen = max(len(ret) for ret in rets)\n",
    "            padded_rets = [np.concatenate([ret, np.zeros(maxlen-len(ret))]) for ret in rets]\n",
    "            # 3. compute time-dependent baseline\n",
    "            baseline = np.mean(padded_rets, axis=0)\n",
    "            # 4. compute advantages\n",
    "            advs = [ret - baseline[:len(ret)] for ret in rets]\n",
    "            all_a = np.concatenate([traj[\"a\"] for traj in trajs])\n",
    "            all_adv = np.concatenate(advs)\n",
    "            # 5. do policy gradient update step\n",
    "            loss = self.f_train(all_s, all_a, all_adv, learning_rate)\n",
    "            train_rs = np.array([traj[\"r\"].sum() for traj in trajs]) # trajectory total rewards\n",
    "            eplens = np.array([len(traj[\"r\"]) for traj in trajs]) # trajectory lengths\n",
    "            # compute validation reward\n",
    "            val_rs = np.array([self.get_trajectory(env, traj_t_limit, deterministic=True)['r'].sum() for _ in range(10)])\n",
    "            # update stats\n",
    "            self.mean_train_rs.append(train_rs.mean())\n",
    "            self.mean_val_rs.append(val_rs.mean())\n",
    "            self.loss.append(loss)\n",
    "            # print stats\n",
    "            print '%3d mean_train_r: %6.2f mean_val_r: %6.2f loss: %f' % (epoch+1, train_rs.mean(), val_rs.mean(), loss)\n",
    "            # render solution\n",
    "            #self.get_trajectory(env, traj_t_limit, render=True)\n",
    "            # check for early stopping: true if the validation reward has not changed in n_early_stop epochs\n",
    "            if n_early_stop and len(self.mean_val_rs) >= n_early_stop and \\\n",
    "                all([x == self.mean_val_rs[-1] for x in self.mean_val_rs[-n_early_stop:-1]]):\n",
    "                break\n",
    "    \n",
    "    def get_trajectory(self, env, t_limit=None, render=False, deterministic=True):\n",
    "        \"\"\"\n",
    "        Compute trajectroy by iteratively evaluating the agent policy on the environment.\n",
    "        \"\"\"\n",
    "        t_limit = t_limit or env.spec.timestep_limit\n",
    "        s = env.reset()\n",
    "        traj = {'s': [], 'a': [], 'r': [], 'f': []}\n",
    "        for _ in xrange(t_limit):\n",
    "            a = self.get_action(s, deterministic)\n",
    "            (s, r, done, _) = env.step(a)\n",
    "            traj['s'].append(s)\n",
    "            traj['a'].append(a)\n",
    "            traj['r'].append(r)\n",
    "            if render: \n",
    "                traj['f'].append(custom_render(s))\n",
    "                if done:\n",
    "                    return {'s': np.array(traj['s']), 'a': np.array(traj['a']), \n",
    "                            'r': np.array(traj['r']), 'f': traj['f']}\n",
    "            if done: break\n",
    "        return {'s': np.array(traj['s']), 'a': np.array(traj['a']), \n",
    "                'r': np.array(traj['r']), 'f': traj['f']}\n",
    "    \n",
    "    def get_action(self, s, deterministic=True):\n",
    "        \"\"\"\n",
    "        Evaluate the agent policy to choose an action, a, given state, s.\n",
    "        \"\"\"\n",
    "        # compute action probabilities\n",
    "        prob_a = self.f_eval(s.reshape(1,-1))\n",
    "        if deterministic:\n",
    "            # choose action with highest probability\n",
    "            return prob_a.argmax()\n",
    "        else:\n",
    "            # sample action from distribution\n",
    "            return (np.cumsum(np.asarray(prob_a)) > np.random.rand()).argmax()\n",
    "    \n",
    "    def _cumulative_discount(self, r, gamma):\n",
    "        \"\"\"\n",
    "        Compute the cumulative discounted rewards (returns).\n",
    "        \"\"\"\n",
    "        r_out = np.zeros(len(r), 'float64')\n",
    "        r_out[-1] = r[-1]\n",
    "        for i in reversed(xrange(len(r)-1)):\n",
    "            r_out[i] = r[i] + gamma * r_out[i+1]\n",
    "        return r_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an agent, let's train it to solve the CartPole task.\n",
    "\n",
    "Note: The agent is not guaranteed to learn a good solution every time as the policy gradient method might get stuck in a local optimum -- you may have to do several restarts to find a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# init agent\n",
    "agent = Agent(n_inputs=env.observation_space.shape[0],\n",
    "              n_outputs=env.action_space.n)\n",
    "# train agent on the environment\n",
    "agent.learn(env, n_epochs=100, learning_rate=0.05, discount_factor=1,\n",
    "            t_per_batch=10000, traj_t_limit=env.spec.timestep_limit, n_early_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot training and validation mean reward\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epochs'); plt.ylabel('mean reward')\n",
    "plt.plot(agent.mean_train_rs, label='training')\n",
    "plt.plot(agent.mean_val_rs, label='validation')\n",
    "plt.xlim((0,len(agent.mean_val_rs)-1))\n",
    "plt.legend(loc=2); plt.grid()\n",
    "_=plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# review solution\n",
    "agent_out = agent.get_trajectory(env, t_limit=1000, render=True)\n",
    "\n",
    "display_frames_as_gif(agent_out['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial solution does not learn to solve the task very well. Here are some hints on how to improve the solution:\n",
    "\n",
    "* Increase the trajectory timestep limit to let the simulations look further into the future.\n",
    "* Increase number of timesteps evaluated per batch.\n",
    "* Try different optimization functions.\n",
    "* Adjust the learning rate.\n",
    "* Adjust the discount factor.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Describe the changes you made and and why they should improve the agent. Are you able to get solutions consistently?\n",
    "2. In the plot above you will sometimes see that the validation reward starts out lower than the training reward but later they cross. How can you explain this behavior?\n",
    "3. Explain step by step the algorithm in the `agent.learn` method with particular attention the points denoted 1-5 in the code above.\n",
    "4. Optional: Monitor and submit your best solution to the Gym (see code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start monitor\n",
    "#env.monitor.start('cartpole-experiment-1')\n",
    "#for _ in xrange(100):\n",
    "#    agent.get_trajectory(env)\n",
    "#env.monitor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have monitored a solution open a Python shell and use the following command to upload the results to OpenAI Gym:\n",
    "\n",
    "```\n",
    "import gym\n",
    "gym.upload('cartpole-experiment-1', api_key='YOUR_API_KEY')\n",
    "```\n",
    "\n",
    "You can also run the command here in the notebook, but remember to remove the API key before handing in the exercise.\n",
    "\n",
    "You can find your API key at your [OpenAI Gym](https://gym.openai.com/) account page. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
